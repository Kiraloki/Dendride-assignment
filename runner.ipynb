{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "4baa6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Session_info\n",
    "# 2 target\n",
    "# 3 train\n",
    "# 4 metrics\n",
    "# 5 feature_handling\n",
    "# 6 feature_generation\n",
    "# 7 feature_reduction\n",
    "# 8 hyperparameters\n",
    "# 9 weighting_stratergy\n",
    "# 10 probability_calibration\n",
    "# 11 algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "76bf86a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from striprtf.striprtf import rtf_to_text \n",
    "\n",
    "json_file_path = \"algoparams_from_ui.json.rtf\"\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    a = rtf_to_text(file.read())\n",
    "    a = a.lower()\n",
    "    parsed = json.loads(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "b89fee2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1 Session_info\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(parsed[\"design_state_data\"][\"session_info\"][\"dataset\"])\n",
    "except:\n",
    "    df = pd.read_csv(\"iris.csv\")\n",
    "    \n",
    "df.columns = [i.strip() for i in df.columns]\n",
    "df.columns = [i.lower() for i in df.columns]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "c04ebdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 target\n",
    "target_column = parsed[\"design_state_data\"][\"target\"].get(\"target\")\n",
    "target_prediction_type = parsed[\"design_state_data\"][\"target\"].get(\"prediction_type\")\n",
    "# independent_variables = parsed[\"design_state_data\"][\"train\"].get(\"time_variable\").split(\",\")\n",
    "independent_variables = [i for i in df.columns if i!=target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "2f1cc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 train\n",
    "from sklearn.model_selection import KFold\n",
    "def kfold_dataset_creator(df, n_splits, randomly, random_state):\n",
    "    if randomly:\n",
    "        kf = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "        ksets = []\n",
    "        for i in kf.split(df.index):\n",
    "            ksets.append(i)\n",
    "        return ksets\n",
    "    else:\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        ksets = []\n",
    "        for i in kf.split(df.index):\n",
    "            ksets.append(i)\n",
    "        return ksets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "bd471437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 3 train\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_test_sets(df, parsed_dict):\n",
    "    if parsed_dict[\"design_state_data\"][\"train\"].get(\"policy\")==\"split the dataset\":\n",
    "        if parsed_dict[\"design_state_data\"][\"train\"].get(\"sampling_method\")==\"no sampling(whole data)\":\n",
    "            if parsed_dict[\"design_state_data\"][\"train\"].get(\"k_fold\")==False or parsed_dict[\"design_state_data\"][\"train\"].get(\"k_fold\")==\"false\":\n",
    "                if 0 < int(parsed_dict[\"design_state_data\"][\"train\"].get(\"train_ratio\")) < 1:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(df[independent_variables], df[target_column],\n",
    "                                                                        train_size=int(parsed_dict[\"design_state_data\"][\"train\"].get(\"train_ratio\")),\n",
    "                                                                       random_state=int(parsed_dict[\"design_state_data\"][\"train\"].get(\"random_seed\")))\n",
    "                    return 1, X_train, X_test, y_train, y_test\n",
    "                else:\n",
    "                    print(\"Changing the train_ratio to 0.8\")\n",
    "                    train_ratio = 0.8\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(df[independent_variables], df[target_column],\n",
    "                                                                        train_size=train_ratio,\n",
    "                                                                       random_state=int(parsed_dict[\"design_state_data\"][\"train\"][\"random_seed\"]))\n",
    "                    return 1, X_train, X_test, y_train, y_test\n",
    "            else:\n",
    "                n_splits = int((train_ratio/(1-train_ratio))+1)\n",
    "                if n_splits<=1:\n",
    "                    print(\"Changing the train_ratio to 0.8\")\n",
    "                    n_splits = 5\n",
    "                randomly = parsed_dict[\"design_state_data\"][\"train\"][\"split\"] == \"randomly\"\n",
    "                ksets = kfold_dataset_creator(df=df, \n",
    "                                              n_splits=n_splits, \n",
    "                                              randomly=randomly, \n",
    "                                              random_state=int(parsed_dict[\"design_state_data\"][\"train\"][\"random_seed\"]))\n",
    "                return 3, ksets\n",
    "        else:\n",
    "            print(\"Create options for data sampling\")\n",
    "    else:\n",
    "        print(\"Training on the whole datatset\")\n",
    "        X_train = df[independent_variables]\n",
    "        y_train = df[target_column]\n",
    "        return 2, X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "f1cb1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standard_scale_column(X_train, X_test, column):\n",
    "    ss = StandardScaler()\n",
    "    scaled_train_set = ss.fit_transform(X_train[column])\n",
    "    try:\n",
    "        scaled_test_set = ss.transform(X_test[column])\n",
    "        return scaled_train_set, scaled_test_set, ss\n",
    "    except:\n",
    "        return scaled_train_set, ss\n",
    "\n",
    "def categorize_column(df_train, df_test, column_name):\n",
    "    unique_values = df_train[column_name].unique()\n",
    "    mapping_dict = {}\n",
    "    for i,j in enumerate(unique_values, 1):\n",
    "        mapping_dict[j] = i\n",
    "    df_train[column_name] = df_train[column_name].map(mapping_dict)\n",
    "    try:\n",
    "        df_test[column_name] = df_test[column_name].map(mapping_dict)\n",
    "        return df_train, df_test, mapping_dict\n",
    "    except:\n",
    "        return df_train, mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "76528608",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 feature_handling\n",
    "def feature_handling(df_train, df_test):\n",
    "    for column in parsed.get(\"design_state_data\").get(\"feature_handling\").keys():\n",
    "        indexer = parsed.get(\"design_state_data\").get(\"feature_handling\").get(column)\n",
    "        if indexer.get(\"is_selected\") or indexer.get(\"is_selected\")==\"true\":\n",
    "            if indexer.get(\"feature_variable_type\")==\"numerical\":\n",
    "                if indexer.get(\"feature_variable_type\")==\"keep as regular numerical feature\":\n",
    "                    if indexer.get(\"feature_details\").get(\"missing_values\")==\"impute\":\n",
    "                        if \"average\" in indexer.get(\"feature_details\").get(\"impute_with\"):\n",
    "                            df_train.get(indexer.get(\"feature_name\")).fillna(df_train.get(indexer.get(\"feature_name\")).mean(), inplace=True)\n",
    "                            df_test.get(indexer.get(\"feature_name\")).fillna(df_test.get(indexer.get(\"feature_name\")).mean(), inplace=True)\n",
    "                            \n",
    "                        if \"forward\" in indexer.get(\"feature_details\").get(\"impute_with\"):\n",
    "                            df_train.get(indexer.get(\"feature_name\")).fillna(method=\"ffill\", inplace=True)\n",
    "                            df_test.get(indexer.get(\"feature_name\")).fillna(method=\"ffill\", inplace=True)\n",
    "                            \n",
    "                        if \"backward\" in indexer.get(\"feature_details\").get(\"impute_with\"):\n",
    "                            df_train.get(indexer.get(\"feature_name\")).fillna(method=\"bfill\", inplace=True)\n",
    "                            df_test.get(indexer.get(\"feature_name\")).fillna(method=\"bfill\", inplace=True)\n",
    "                            \n",
    "                        else:\n",
    "                            df_train.get(indexer.get(\"feature_name\")).fillna(indexer.get(\"feature_details\").get(\"impute_value\"), inplace=True)\n",
    "                            df_test.get(indexer.get(\"feature_name\")).fillna(indexer.get(\"feature_details\").get(\"impute_value\"), inplace=True)\n",
    "                    if indexer.get(\"feature_details\").get(\"rescaling\"):\n",
    "                        df_train, df_test, ss = standard_scale_indexer(df_train, df_test, column)\n",
    "                        \n",
    "            elif indexer.get(\"feature_variable_type\")==\"text\":\n",
    "                if indexer.get(\"feature_details\").get(\"text_handling\")==\"tokenize and hash\":\n",
    "                    df_train, df_test, mapping_dict = categorize_column(df_train, df_test, indexer.get(\"feature_name\"))\n",
    "\n",
    "            else:\n",
    "                print(f\"Not a numerical feature, hence skipping feature handling for {indexer.get('feature_name')}\")\n",
    "        else:\n",
    "            df_train = df_train[[i for i in df_train.columns if i!=indexer.get(\"feature_name\")]]\n",
    "            df_test = df_test[[i for i in df_test.columns if i!=indexer.get(\"feature_name\")]]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "8385a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_handling(df, parsed):\n",
    "    train_test_return = train_test_sets(df, parsed)\n",
    "    if train_test_return[0]==1:\n",
    "        X_train, X_test, y_train, y_test = train_test_return[1], train_test_return[2], train_test_return[3], train_test_return[4]\n",
    "        df_train = X_train.merge(y_train, left_index=True, right_index=True)\n",
    "        df_test = X_test.merge(y_test, left_index=True, right_index=True)\n",
    "        df_train, df_test = feature_handling(df_train, df_test)\n",
    "        return 1, df_train, df_test\n",
    "\n",
    "    elif train_test_return[0]==2:\n",
    "        X_train, y_train = train_test_return[1], train_test_return[2]\n",
    "        df_train = X_train.merge(y_train, left_index=True, right_index=True)\n",
    "        df_train = feature_handling(df_train, -1)\n",
    "        return 2, df_train\n",
    "\n",
    "    elif train_test_return[0]==3:\n",
    "        ksets = train_test_return[1]\n",
    "        train_test_sets_final = []\n",
    "        for i, j in enumerate(ksets):\n",
    "            df_train, df_test = df.loc[j[0]], df.loc[j[1]]\n",
    "            df_train, dftest = feature_handling(df_train, dftest)\n",
    "            train_test_sets.append((df_train, df_test))\n",
    "        return 3, train_test_sets_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "c3e86908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the train_ratio to 0.8\n"
     ]
    }
   ],
   "source": [
    "processed_data = process_feature_handling(df, parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "d385f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "if processed_data[0]==1:\n",
    "    df_train, df_test = processed_data[1], processed_data[2]\n",
    "elif processed_data[0]==2:\n",
    "    df_train = processed_data[1]\n",
    "elif processed_data[0]==3:\n",
    "    ### need to perform\n",
    "    print(\"Multiple iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "dd42141c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>species</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  species  petal_width\n",
       "137           6.4          3.1           5.5        1          1.8\n",
       "84            5.4          3.0           4.5        2          1.5\n",
       "27            5.2          3.5           1.5        3          0.2\n",
       "127           6.1          3.0           4.9        1          1.8\n",
       "132           6.4          2.8           5.6        1          2.2"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "661d6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "def feature_reduction(df_train, df_test, parsed):\n",
    "    parsed = parsed.get(\"design_state_data\")\n",
    "    if \"{\" in str(parsed[\"feature_reduction\"].values()):\n",
    "        list_of_methods = list(parsed[\"feature_reduction\"].keys())[1:]\n",
    "        for each_method in list_of_methods:\n",
    "            if parsed.get(\"feature_reduction\").get(each_method).get(\"is_selected\"):\n",
    "                if \"no reduction\" in each_method:\n",
    "                    print(\"No reduction being done\")\n",
    "                    break\n",
    "                elif \"correlation\" in each_method:\n",
    "                    corr_matrix = df_train.corr()\n",
    "                    columns_to_keep = list(corr_matrix.iloc[:,-1].index)\n",
    "                    corr_matrix = df_train.corr()\n",
    "                    vals = [i if i>=0 else -1*i for i in corr_matrix[target_column].values]\n",
    "                    corr_column_names = list(df_train.corr().columns)\n",
    "                    df_corr_dict = pd.DataFrame({\"correlation_value\": vals, \"column_name\": corr_column_names})\n",
    "                    df_corr_dict.sort_values('correlation_value')\n",
    "                    required_count = parsed.get(\"feature_reduction\").get(\"correlation with target\").get(\"num_of_features_to_keep\")\n",
    "                    required_columns = df_corr_dict.iloc[:required_count-1, -1]\n",
    "                    required_columns.append(target_column)\n",
    "                    df_train = df_train[required_columns]\n",
    "                    try:\n",
    "                        df_test = df_test[required_columns]\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"tree-based\" in each_method:\n",
    "                    print(\"tree based reduction\")\n",
    "                elif \"principal component analysis\" in each_method:\n",
    "                    pca = decomposition.PCA(n_components=parsed.get(\"feature_reduction\").get(\"principal component analysis\").get(\"number_of_features_to_keep\"))\n",
    "                    y_train = df_train.loc[:,target_column]\n",
    "                    df_train = pd.DataFrame(pca.fit_transform(df_train[[i for i in df.columns if i!=target_column]]))\n",
    "                    df_train = df_train.merge(y_train, left_index=True, right_index=True)\n",
    "                    df_train.columns[-1] = target_column\n",
    "                    try:\n",
    "                        y_test = df_test.loc[:,target_column]\n",
    "                        df_test = pd.DataFrame(pca.transform(df_test[[i for i in df.columns if i!=target_column]]))\n",
    "                        df_test = df_test.merge(y_test, left_index=True, right_index=True)\n",
    "                        df_test.columns[-1] = target_column\n",
    "                    except:\n",
    "                        pass                    \n",
    "    else:\n",
    "        print(\"Not specified\")\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "5f54705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not specified\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = feature_reduction(df_train, df_test, parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "50e283f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {\"classification\": [\"randomforestclassifier\",\n",
    "                            \"gbtclassifier\",\n",
    "                            \"decisiontreeclassifier\",\n",
    "                            \"logisticregression\",\n",
    "                            \"knn\"],\n",
    "             \"regression\": [\"randomforestregressor\",\n",
    "                          \"gbtregressor\",\n",
    "                          \"ridgeregression\",\n",
    "                           \"linearregression\",\n",
    "                           \"lassoregression\",\n",
    "                           \"elasticnetregression\",\n",
    "                           \"xgboost\",\n",
    "                           \"decisiontreeregressor\",\n",
    "                           \"svm\",\n",
    "                           \"sgd\",\n",
    "                           \"extrarandomtrees\",\n",
    "                           \"neuralnetwork\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "4e77a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBosstingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def train_models(list_of_models, parsed, df_train, df_test):\n",
    "    trained_models = []\n",
    "    if \"randomforestregressor\" in list_of_models:\n",
    "        rfr_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"randomforestregressor\")\n",
    "        rfr = RandomForestRegressor(n_estimators= rfr_dict.get(\"max_trees\"), \n",
    "                                    max_depth=rfr_dict.get(\"max_depth\"), \n",
    "                                    criterion=rfr_dict.get(\"feature_sampling_stratergy\"))\n",
    "        rfr.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(rfr)\n",
    "    if \"gbtregressor\" in list_of_models:\n",
    "        gbtr_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"gbtregressor\")\n",
    "        gbr = GradientBoostingRegressor(n_estimators=gbtr_dict.get(\"num_of_BoostingStages\") max_depth=\"max_depth\")\n",
    "        gbr.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(gbr)\n",
    "    if \"ridgeregression\" in list_of_models:\n",
    "        rdg_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"ridgeregression\")\n",
    "        rdg = Ridge(max_iter=rdg_dict[\"max_iter\"])\n",
    "        rdg.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(rdg)\n",
    "    if \"linearregression\" in list_of_models:\n",
    "        rdg_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"linearregression\")\n",
    "        lnreg = LinearRegression(n_jobs=n_jobs)\n",
    "        lnreg.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(lnreg)\n",
    "    if \"lassoregression\" in list_of_models:\n",
    "        laso_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"lassoregression\")\n",
    "        laso = Lasso(max_tier=laso_dict[\"max_iter\"])\n",
    "        laso.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(laso)\n",
    "    if \"elasticnetregression\" in list_of_models:\n",
    "        enet_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"elasticnetregression\")\n",
    "        enet = ElasticNet(max_tier=enet_dict[\"max_tier\"])\n",
    "        enet.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(enet)\n",
    "    if \"decisiontreeregressor\" in list_of_models:\n",
    "        dtr_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"decisiontreeregressor\")\n",
    "        dtr = DecisionTreeRegressor(max_depth=dtr_dict[\"max_depth\"])\n",
    "        dtr.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(dtr)\n",
    "    if \"svm\" in list_of_models:\n",
    "        svr_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"svm\")\n",
    "        svr = SVR(kernel=\"linear_kernel\")\n",
    "        svr = DecisionTreeRegressor(max_depth=svr_dict[\"max_depth\"])\n",
    "        dtr.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(dtr)\n",
    "    if \"sgd\" in list_of_models:\n",
    "        sgdreg_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"sgd\")\n",
    "        sgdreg = SGDRegressor(alpha=sgdreg_dict[\"alpha\"])\n",
    "        sgdreg.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(sgdreg)\n",
    "    if \"extra_random_trees\" in list_of_models:\n",
    "        etreg_dict = parsed.get(\"design_state_data\").get(\"algorithms\").get(\"extra_random_trees\")\n",
    "        etreg = ExtraTreesRegressor(n_estimators=etreg_dict[\"num_of_trees\"])\n",
    "        etreg.fit(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "        trained_models.append(etreg)\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2f3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
